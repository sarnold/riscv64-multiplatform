From 9151393c5aa0c049d3f09d863e0da05b1d30ad35 Mon Sep 17 00:00:00 2001
From: Tom <support@vamrs.com>
Date: Fri, 8 Jan 2021 04:01:19 +0800
Subject: [PATCH 48/79] Add dcache flush functions in stmicro stmmac driver for
 VIC7100

---
 .../net/ethernet/stmicro/stmmac/stmmac_main.c | 210 +++++++++++++++++-
 1 file changed, 207 insertions(+), 3 deletions(-)

diff --git a/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c b/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
index c33db79cdd0a..cff104c76c32 100644
--- a/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
+++ b/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
@@ -113,6 +113,20 @@ static void stmmac_exit_fs(struct net_device *dev);
 
 #define STMMAC_COAL_TIMER(x) (jiffies + usecs_to_jiffies(x))
 
+#ifdef CONFIG_FPGA_GMAC_FLUSH_DDR
+#define FLUSH_RX_DESC_ENABLE
+#define FLUSH_RX_BUF_ENABLE
+
+#define FLUSH_TX_DESC_ENABLE
+#define FLUSH_TX_BUF_ENABLE
+
+#include <soc/starfive/vic7100.h>
+static inline void stmmac_flush_dcache(unsigned long start, unsigned long len)
+{
+	starfive_flush_dcache(_ALIGN_DOWN(start, 64), len + start % 64);
+}
+#endif
+
 /**
  * stmmac_verify_args - verify the driver parameters.
  * Description: it checks the driver parameters and set a default in case of
@@ -1202,6 +1216,16 @@ static void stmmac_clear_rx_descriptors(struct stmmac_priv *priv, u32 queue)
 					priv->use_riwt, priv->mode,
 					(i == priv->dma_rx_size - 1),
 					priv->dma_buf_sz);
+
+#ifdef FLUSH_RX_DESC_ENABLE
+	unsigned long len;
+	if (priv->extend_desc)
+		len = DMA_DEFAULT_RX_SIZE * sizeof(struct dma_extended_desc);
+	else
+		len = DMA_DEFAULT_RX_SIZE * sizeof(struct dma_desc);
+
+	stmmac_flush_dcache(rx_q->dma_rx_phy, len);
+#endif
 }
 
 /**
@@ -1230,6 +1254,16 @@ static void stmmac_clear_tx_descriptors(struct stmmac_priv *priv, u32 queue)
 
 		stmmac_init_tx_desc(priv, p, priv->mode, last);
 	}
+
+#ifdef FLUSH_TX_DESC_ENABLE
+	unsigned long len;
+	if (priv->extend_desc)
+		len = DMA_TX_SIZE * sizeof(struct dma_extended_desc);
+	else
+		len = DMA_TX_SIZE * sizeof(struct dma_desc);
+
+	stmmac_flush_dcache(tx_q->dma_tx_phy, len);
+#endif
 }
 
 /**
@@ -1286,6 +1320,9 @@ static int stmmac_init_rx_buffers(struct stmmac_priv *priv, struct dma_desc *p,
 
 	buf->addr = page_pool_get_dma_addr(buf->page);
 	stmmac_set_desc_addr(priv, p, buf->addr);
+#ifdef FLUSH_RX_BUF_ENABLE
+	stmmac_flush_dcache(buf->addr, priv->dma_buf_sz);
+#endif
 	if (priv->dma_buf_sz == BUF_SIZE_16KiB)
 		stmmac_init_desc3(priv, p);
 
@@ -1469,6 +1506,15 @@ static int init_dma_tx_desc_rings(struct net_device *dev)
 			tx_q->tx_skbuff_dma[i].last_segment = false;
 			tx_q->tx_skbuff[i] = NULL;
 		}
+#ifdef FLUSH_TX_DESC_ENABLE
+		unsigned long len;
+		if (priv->extend_desc)
+			len = DMA_TX_SIZE * sizeof(struct dma_extended_desc);
+		else
+			len = DMA_TX_SIZE * sizeof(struct dma_desc);
+
+		stmmac_flush_dcache(tx_q->dma_tx_phy, len);
+#endif
 
 		tx_q->dirty_tx = 0;
 		tx_q->cur_tx = 0;
@@ -1990,7 +2036,21 @@ static int stmmac_tx_clean(struct stmmac_priv *priv, int budget, u32 queue)
 				&priv->xstats, p, priv->ioaddr);
 		/* Check if the descriptor is owned by the DMA */
 		if (unlikely(status & tx_dma_own))
+		{
+#ifdef FLUSH_TX_DESC_ENABLE
+		unsigned long start, len;
+		if (priv->extend_desc) {
+			start = tx_q->dma_tx_phy + entry * sizeof(struct dma_extended_desc);
+			len = sizeof(struct dma_extended_desc);
+		} else {
+			start = tx_q->dma_tx_phy + entry * sizeof(struct dma_desc);
+			len = sizeof(struct dma_desc);
+		}
+
+		stmmac_flush_dcache(start, len);
+#endif
 			break;
+		}
 
 		count++;
 
@@ -2040,6 +2100,19 @@ static int stmmac_tx_clean(struct stmmac_priv *priv, int budget, u32 queue)
 		}
 
 		stmmac_release_tx_desc(priv, p, priv->mode);
+#ifdef FLUSH_TX_DESC_ENABLE
+		/*wangyh for test,flush description*/
+		unsigned long start, len;
+		if (priv->extend_desc) {
+			start = tx_q->dma_tx_phy + entry * sizeof(struct dma_extended_desc);
+			len = sizeof(struct dma_extended_desc);
+		} else {
+			start = tx_q->dma_tx_phy + entry * sizeof(struct dma_desc);
+			len = sizeof(struct dma_desc);
+		}
+
+		stmmac_flush_dcache(start, len);
+#endif
 
 		entry = STMMAC_GET_ENTRY(entry, priv->dma_tx_size);
 	}
@@ -2087,6 +2160,16 @@ static void stmmac_tx_err(struct stmmac_priv *priv, u32 chan)
 	stmmac_stop_tx_dma(priv, chan);
 	dma_free_tx_skbufs(priv, chan);
 	stmmac_clear_tx_descriptors(priv, chan);
+
+#ifdef FLUSH_TX_DESC_ENABLE
+	unsigned long len;
+	if (priv->extend_desc)
+		len = DMA_TX_SIZE * sizeof(struct dma_extended_desc);
+	else
+		len = DMA_TX_SIZE * sizeof(struct dma_desc);
+
+	stmmac_flush_dcache(tx_q->dma_tx_phy, len);
+#endif
 	tx_q->dirty_tx = 0;
 	tx_q->cur_tx = 0;
 	tx_q->mss = 0;
@@ -3025,6 +3108,18 @@ static void stmmac_tso_allocator(struct stmmac_priv *priv, dma_addr_t des,
 				(last_segment) && (tmp_len <= TSO_MAX_BUFF_SIZE),
 				0, 0);
 
+#ifdef FLUSH_TX_DESC_ENABLE
+		unsigned long start, len;
+		if (priv->extend_desc) {
+			start = tx_q->dma_tx_phy + tx_q->cur_tx * sizeof(struct dma_extended_desc);
+			len = sizeof(struct dma_extended_desc);
+		} else {
+			start = tx_q->dma_tx_phy + tx_q->cur_tx * sizeof(struct dma_desc);
+			len = sizeof(struct dma_desc);
+		}
+
+		stmmac_flush_dcache(start, len);
+#endif
 		tmp_len -= TSO_MAX_BUFF_SIZE;
 	}
 }
@@ -3070,6 +3165,9 @@ static netdev_tx_t stmmac_tso_xmit(struct sk_buff *skb, struct net_device *dev)
 	u32 pay_len, mss;
 	dma_addr_t des;
 	int i;
+#ifdef FLUSH_TX_DESC_ENABLE
+	unsigned int mss_entry;
+#endif
 
 	tx_q = &priv->tx_queue[queue];
 	first_tx = tx_q->cur_tx;
@@ -3109,6 +3207,9 @@ static netdev_tx_t stmmac_tso_xmit(struct sk_buff *skb, struct net_device *dev)
 			mss_desc = &tx_q->dma_tx[tx_q->cur_tx];
 
 		stmmac_set_mss(priv, mss_desc, mss);
+#ifdef FLUSH_TX_DESC_ENABLE
+		mss_entry = tx_q->cur_tx;
+#endif
 		tx_q->mss = mss;
 		tx_q->cur_tx = STMMAC_GET_ENTRY(tx_q->cur_tx,
 						priv->dma_tx_size);
@@ -3143,6 +3244,10 @@ static netdev_tx_t stmmac_tso_xmit(struct sk_buff *skb, struct net_device *dev)
 	if (dma_mapping_error(priv->device, des))
 		goto dma_map_err;
 
+#ifdef FLUSH_TX_BUF_ENABLE
+	stmmac_flush_dcache(des, skb_headlen(skb));
+#endif
+
 	tx_q->tx_skbuff_dma[first_entry].buf = des;
 	tx_q->tx_skbuff_dma[first_entry].len = skb_headlen(skb);
 
@@ -3174,6 +3279,9 @@ static netdev_tx_t stmmac_tso_xmit(struct sk_buff *skb, struct net_device *dev)
 		if (dma_mapping_error(priv->device, des))
 			goto dma_map_err;
 
+#ifdef FLUSH_TX_BUF_ENABLE
+		stmmac_flush_dcache(des, skb_frag_size(frag));
+#endif
 		stmmac_tso_allocator(priv, des, skb_frag_size(frag),
 				     (i == nfrags - 1), queue);
 
@@ -3218,7 +3326,6 @@ static netdev_tx_t stmmac_tso_xmit(struct sk_buff *skb, struct net_device *dev)
 	 * ndo_start_xmit will fill this descriptor the next time it's
 	 * called and stmmac_tx_clean may clean up to this descriptor.
 	 */
-	tx_q->cur_tx = STMMAC_GET_ENTRY(tx_q->cur_tx, priv->dma_tx_size);
 
 	if (unlikely(stmmac_tx_avail(priv, queue) <= (MAX_SKB_FRAGS + 1))) {
 		netif_dbg(priv, hw, priv->dev, "%s: stop transmitted packets\n",
@@ -3249,6 +3356,18 @@ static netdev_tx_t stmmac_tso_xmit(struct sk_buff *skb, struct net_device *dev)
 			1, tx_q->tx_skbuff_dma[first_entry].last_segment,
 			hdr / 4, (skb->len - proto_hdr_len));
 
+#ifdef FLUSH_TX_DESC_ENABLE
+	unsigned long start, len;
+	if (priv->extend_desc) {
+		start = tx_q->dma_tx_phy + first_entry * sizeof(struct dma_extended_desc);
+		len = sizeof(struct dma_extended_desc);
+	} else {
+		start = tx_q->dma_tx_phy + first_entry * sizeof(struct dma_desc);
+		len = sizeof(struct dma_desc);
+	}
+
+	stmmac_flush_dcache(start, len);
+#endif
 	/* If context desc is used to change MSS */
 	if (mss_desc) {
 		/* Make sure that first descriptor has been completely
@@ -3258,6 +3377,18 @@ static netdev_tx_t stmmac_tso_xmit(struct sk_buff *skb, struct net_device *dev)
 		 */
 		dma_wmb();
 		stmmac_set_tx_owner(priv, mss_desc);
+#ifdef FLUSH_TX_DESC_ENABLE
+		unsigned long start, len;
+		if (priv->extend_desc) {
+			start = tx_q->dma_tx_phy + mss_entry * sizeof(struct dma_extended_desc);
+			len = sizeof(struct dma_extended_desc);
+		} else {
+			start = tx_q->dma_tx_phy + mss_entry * sizeof(struct dma_desc);
+			len = sizeof(struct dma_desc);
+		}
+
+		stmmac_flush_dcache(start, len);
+#endif
 	}
 
 	/* The own bit must be the latest setting done when prepare the
@@ -3396,6 +3527,9 @@ static netdev_tx_t stmmac_xmit(struct sk_buff *skb, struct net_device *dev)
 		if (dma_mapping_error(priv->device, des))
 			goto dma_map_err; /* should reuse desc w/o issues */
 
+#ifdef FLUSH_TX_BUF_ENABLE
+		 stmmac_flush_dcache(des, len);
+#endif
 		tx_q->tx_skbuff_dma[entry].buf = des;
 
 		stmmac_set_desc_addr(priv, desc, des);
@@ -3407,6 +3541,18 @@ static netdev_tx_t stmmac_xmit(struct sk_buff *skb, struct net_device *dev)
 		/* Prepare the descriptor and set the own bit too */
 		stmmac_prepare_tx_desc(priv, desc, 0, len, csum_insertion,
 				priv->mode, 1, last_segment, skb->len);
+#ifdef FLUSH_TX_DESC_ENABLE
+		unsigned long start, desc_len;
+		if (priv->extend_desc) {
+			start = tx_q->dma_tx_phy + entry * sizeof(struct dma_extended_desc);
+			desc_len = sizeof(struct dma_extended_desc);
+		} else {
+			start = tx_q->dma_tx_phy + entry * sizeof(struct dma_desc);
+			desc_len = sizeof(struct dma_desc);
+		}
+
+		stmmac_flush_dcache(start, desc_len);
+#endif
 	}
 
 	/* Only the last descriptor gets to point to the skb. */
@@ -3441,6 +3587,18 @@ static netdev_tx_t stmmac_xmit(struct sk_buff *skb, struct net_device *dev)
 
 		tx_q->tx_count_frames = 0;
 		stmmac_set_tx_ic(priv, desc);
+#ifdef FLUSH_TX_DESC_ENABLE
+		unsigned long start, len;
+		if (priv->extend_desc) {
+			start = tx_q->dma_tx_phy + entry * sizeof(struct dma_extended_desc);
+			len = sizeof(struct dma_extended_desc);
+		} else {
+			start = tx_q->dma_tx_phy + entry * sizeof(struct dma_desc);
+			len = sizeof(struct dma_desc);
+		}
+
+		stmmac_flush_dcache(start, len);
+#endif
 		priv->xstats.tx_set_ic_bit++;
 	}
 
@@ -3524,6 +3682,23 @@ static netdev_tx_t stmmac_xmit(struct sk_buff *skb, struct net_device *dev)
 
 	netdev_tx_sent_queue(netdev_get_tx_queue(dev, queue), skb->len);
 
+#ifdef FLUSH_TX_BUF_ENABLE
+	stmmac_flush_dcache(des, nopaged_len);
+#endif
+
+#ifdef FLUSH_TX_DESC_ENABLE
+	unsigned long start, len;
+	if (priv->extend_desc) {
+		start = tx_q->dma_tx_phy + first_entry * sizeof(struct dma_extended_desc);
+		len = sizeof(struct dma_extended_desc);
+	} else {
+		start = tx_q->dma_tx_phy + first_entry * sizeof(struct dma_desc);
+		len = sizeof(struct dma_desc);
+	}
+
+	stmmac_flush_dcache(start, len);
+#endif
+
 	stmmac_enable_dma_transmission(priv, priv->ioaddr);
 
 	if (likely(priv->extend_desc))
@@ -3533,6 +3708,9 @@ static netdev_tx_t stmmac_xmit(struct sk_buff *skb, struct net_device *dev)
 	else
 		desc_size = sizeof(struct dma_desc);
 
+	entry = STMMAC_GET_ENTRY(entry, DMA_TX_SIZE);
+	tx_q->cur_tx = entry;
+
 	tx_q->tx_tail_addr = tx_q->dma_tx_phy + (tx_q->cur_tx * desc_size);
 	stmmac_set_tx_tail_ptr(priv, priv->ioaddr, tx_q->tx_tail_addr, queue);
 	stmmac_tx_timer_arm(priv, queue);
@@ -3631,9 +3809,22 @@ static inline void stmmac_rx_refill(struct stmmac_priv *priv, u32 queue)
 		if (!priv->use_riwt)
 			use_rx_wd = false;
 
-		dma_wmb();
 		stmmac_set_rx_owner(priv, p, use_rx_wd);
 
+#ifdef FLUSH_RX_DESC_ENABLE
+		unsigned long start, len;
+		if (priv->extend_desc) {
+			start = rx_q->dma_rx_phy + entry * sizeof(struct dma_extended_desc);
+			len = sizeof(struct dma_extended_desc);
+		} else {
+			start = rx_q->dma_rx_phy + entry * sizeof(struct dma_desc);
+			len = sizeof(struct dma_desc);
+		}
+
+		stmmac_flush_dcache(start, len);
+#endif
+		dma_wmb();
+
 		entry = STMMAC_GET_ENTRY(entry, priv->dma_rx_size);
 	}
 	rx_q->dirty_rx = entry;
@@ -3756,8 +3947,21 @@ static int stmmac_rx(struct stmmac_priv *priv, int limit, u32 queue)
 		status = stmmac_rx_status(priv, &priv->dev->stats,
 				&priv->xstats, p);
 		/* check if managed by the DMA otherwise go ahead */
-		if (unlikely(status & dma_own))
+		if (unlikely(status & dma_own)) {
+#ifdef FLUSH_RX_DESC_ENABLE
+		unsigned long start, len;
+		if (priv->extend_desc) {
+			start = rx_q->dma_rx_phy + entry * sizeof(struct dma_extended_desc);
+			len = sizeof(struct dma_extended_desc);
+		} else {
+			start = rx_q->dma_rx_phy + entry * sizeof(struct dma_desc);
+			len = sizeof(struct dma_desc);
+		}
+
+		stmmac_flush_dcache(start, len);
+#endif
 			break;
+		}
 
 		rx_q->cur_rx = STMMAC_GET_ENTRY(rx_q->cur_rx,
 						priv->dma_rx_size);
-- 
2.30.0

